{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54c0ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "#import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "#Components used for signal - ACF,PSD\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba0b6934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following files : \n",
      "WindowedPSD-Part1.2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinoth\\AppData\\Local\\Temp\\ipykernel_7860\\1994896137.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  empty  = empty.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindowedPSD-Part1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinoth\\AppData\\Local\\Temp\\ipykernel_7860\\1994896137.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  empty  = empty.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindowedPSD-Part3.2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinoth\\AppData\\Local\\Temp\\ipykernel_7860\\1994896137.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  empty  = empty.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindowedPSD-Part3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinoth\\AppData\\Local\\Temp\\ipykernel_7860\\1994896137.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  empty  = empty.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindowedPSD-Part4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinoth\\AppData\\Local\\Temp\\ipykernel_7860\\1994896137.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  empty  = empty.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindowedPSD-Part5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinoth\\AppData\\Local\\Temp\\ipykernel_7860\\1994896137.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  empty  = empty.append(df1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>600</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.024759e-07</td>\n",
       "      <td>3.453213e-07</td>\n",
       "      <td>4.516516e-07</td>\n",
       "      <td>3.525591e-07</td>\n",
       "      <td>4.475218e-07</td>\n",
       "      <td>4.822920e-07</td>\n",
       "      <td>5.180486e-07</td>\n",
       "      <td>9.011292e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.395725e-07</td>\n",
       "      <td>9.887520e-07</td>\n",
       "      <td>9.368137e-07</td>\n",
       "      <td>1.348996e-06</td>\n",
       "      <td>1.343495e-06</td>\n",
       "      <td>1.200532e-06</td>\n",
       "      <td>1.408497e-06</td>\n",
       "      <td>1.464367e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.891354e-06</td>\n",
       "      <td>2.536069e-06</td>\n",
       "      <td>1.358203e-06</td>\n",
       "      <td>9.048994e-07</td>\n",
       "      <td>1.994346e-06</td>\n",
       "      <td>2.360339e-06</td>\n",
       "      <td>1.342208e-06</td>\n",
       "      <td>1.691815e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.188600e-06</td>\n",
       "      <td>7.858382e-07</td>\n",
       "      <td>1.804617e-06</td>\n",
       "      <td>2.639991e-06</td>\n",
       "      <td>1.755323e-06</td>\n",
       "      <td>1.311572e-06</td>\n",
       "      <td>2.520165e-06</td>\n",
       "      <td>2.858946e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.139918e-06</td>\n",
       "      <td>1.122375e-06</td>\n",
       "      <td>8.596644e-07</td>\n",
       "      <td>8.723769e-07</td>\n",
       "      <td>7.107593e-07</td>\n",
       "      <td>9.061113e-07</td>\n",
       "      <td>9.147982e-07</td>\n",
       "      <td>1.096754e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            600           601           602           603           604  \\\n",
       "0  3.024759e-07  3.453213e-07  4.516516e-07  3.525591e-07  4.475218e-07   \n",
       "1  8.395725e-07  9.887520e-07  9.368137e-07  1.348996e-06  1.343495e-06   \n",
       "2  1.891354e-06  2.536069e-06  1.358203e-06  9.048994e-07  1.994346e-06   \n",
       "3  1.188600e-06  7.858382e-07  1.804617e-06  2.639991e-06  1.755323e-06   \n",
       "4  1.139918e-06  1.122375e-06  8.596644e-07  8.723769e-07  7.107593e-07   \n",
       "\n",
       "            605           606           607       608       609  ...  \\\n",
       "0  4.822920e-07  5.180486e-07  9.011292e-07  0.000001  0.000001  ...   \n",
       "1  1.200532e-06  1.408497e-06  1.464367e-06  0.000002  0.000002  ...   \n",
       "2  2.360339e-06  1.342208e-06  1.691815e-06  0.000003  0.000002  ...   \n",
       "3  1.311572e-06  2.520165e-06  2.858946e-06  0.000002  0.000001  ...   \n",
       "4  9.061113e-07  9.147982e-07  1.096754e-06  0.000001  0.000002  ...   \n",
       "\n",
       "        991       992       993       994       995       996       997  \\\n",
       "0  0.000003  0.000003  0.000003  0.000003  0.000003  0.000003  0.000004   \n",
       "1  0.000003  0.000004  0.000004  0.000004  0.000004  0.000004  0.000003   \n",
       "2  0.000003  0.000003  0.000003  0.000003  0.000003  0.000004  0.000004   \n",
       "3  0.000003  0.000004  0.000004  0.000004  0.000004  0.000004  0.000004   \n",
       "4  0.000003  0.000004  0.000004  0.000004  0.000003  0.000003  0.000003   \n",
       "\n",
       "        998       999  Label  \n",
       "0  0.000004  0.000004      0  \n",
       "1  0.000004  0.000004      0  \n",
       "2  0.000003  0.000003      0  \n",
       "3  0.000003  0.000004      0  \n",
       "4  0.000003  0.000003      0  \n",
       "\n",
       "[5 rows x 401 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting current working directory and storing it as a string variable and reading all empty seat readings into one dataframe\n",
    "\n",
    "directory_in_str = \"E:\\\\Haritha\\\\MasterThesis\\\\ModelTraining\\\\aa_measurements\\\\FFTfilegenerationpurpose\\\\psd-windowed-train(adc-acf-window-fft)\"\n",
    "Emptyfilelist = []\n",
    "empty = pd.DataFrame()\n",
    "print(\"Adding the following files : \")\n",
    "for file in os.listdir(directory_in_str) :\n",
    "    if file.startswith(\"Windowed\") :\n",
    "        print(file)\n",
    "        filepathempty = directory_in_str +\"\\\\\" + file\n",
    "        df1 = pd.read_csv(filepathempty, engine = 'python')\n",
    "        empty  = empty.append(df1)\n",
    "    else :\n",
    "        continue  \n",
    "\n",
    "empty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "068de882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinoth\\AppData\\Local\\Temp\\ipykernel_7860\\3556709548.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  total2 = total2.append(empty)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15280, 401)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total2 = pd.DataFrame()\n",
    "total2 = total2.append(empty)\n",
    "total2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d3da3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following files : \n",
      "WindowedPSD-Part2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinoth\\AppData\\Local\\Temp\\ipykernel_7860\\643850159.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  testing  = testing.append(df2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>600</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        600       601       602       603       604       605       606  \\\n",
       "0  0.000002  0.000003  0.000003  0.000004  0.000005  0.000005  0.000006   \n",
       "1  0.000004  0.000005  0.000005  0.000006  0.000007  0.000008  0.000010   \n",
       "2  0.000006  0.000004  0.000004  0.000004  0.000003  0.000002  0.000003   \n",
       "3  0.000004  0.000005  0.000007  0.000008  0.000008  0.000011  0.000011   \n",
       "4  0.000007  0.000006  0.000008  0.000006  0.000006  0.000009  0.000008   \n",
       "\n",
       "        607       608       609  ...       991       992       993       994  \\\n",
       "0  0.000008  0.000010  0.000011  ...  0.000007  0.000007  0.000007  0.000006   \n",
       "1  0.000011  0.000012  0.000013  ...  0.000007  0.000006  0.000006  0.000006   \n",
       "2  0.000002  0.000002  0.000002  ...  0.000004  0.000004  0.000003  0.000004   \n",
       "3  0.000010  0.000011  0.000012  ...  0.000007  0.000006  0.000006  0.000006   \n",
       "4  0.000011  0.000015  0.000011  ...  0.000007  0.000007  0.000006  0.000006   \n",
       "\n",
       "        995       996       997       998       999  Label  \n",
       "0  0.000006  0.000005  0.000005  0.000005  0.000004      0  \n",
       "1  0.000005  0.000005  0.000004  0.000005  0.000004      0  \n",
       "2  0.000004  0.000003  0.000003  0.000004  0.000003      0  \n",
       "3  0.000005  0.000005  0.000004  0.000004  0.000004      0  \n",
       "4  0.000005  0.000005  0.000005  0.000004  0.000004      0  \n",
       "\n",
       "[5 rows x 401 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_in_str_test = \"E:\\\\Haritha\\\\MasterThesis\\\\ModelTraining\\\\aa_measurements\\\\FFTfilegenerationpurpose\\\\psd-windowed_test(adc-acf-window-fft)\"\n",
    "Emptyfilelist = []\n",
    "testing = pd.DataFrame()\n",
    "print(\"Adding the following files : \")\n",
    "for file in os.listdir(directory_in_str_test) :\n",
    "    if file.startswith(\"Windowed\") :\n",
    "        print(file)\n",
    "        filepathtest = directory_in_str_test +\"\\\\\" + file\n",
    "        df2 = pd.read_csv(filepathtest, engine = 'python')\n",
    "        testing  = testing.append(df2)\n",
    "    else :\n",
    "        continue  \n",
    "\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "205e0114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15280"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthvar = len(total2)\n",
    "lengthvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f503c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(total2, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c084fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3056, 401)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8725b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:,0:400] \n",
    "y_train = train['Label']\n",
    "\n",
    "X_test = test.iloc[:,0:400] \n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c561426e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>600</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1.162032e-06</td>\n",
       "      <td>1.426706e-06</td>\n",
       "      <td>3.316030e-06</td>\n",
       "      <td>3.999727e-06</td>\n",
       "      <td>2.659342e-06</td>\n",
       "      <td>1.188642e-06</td>\n",
       "      <td>1.898837e-06</td>\n",
       "      <td>4.202299e-06</td>\n",
       "      <td>5.332507e-06</td>\n",
       "      <td>3.882308e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>5.100069e-06</td>\n",
       "      <td>2.961782e-06</td>\n",
       "      <td>5.754323e-06</td>\n",
       "      <td>6.631708e-06</td>\n",
       "      <td>5.013103e-06</td>\n",
       "      <td>8.982251e-06</td>\n",
       "      <td>8.297434e-06</td>\n",
       "      <td>1.184840e-05</td>\n",
       "      <td>8.661218e-06</td>\n",
       "      <td>2.002770e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>2.228858e-06</td>\n",
       "      <td>2.319863e-06</td>\n",
       "      <td>2.620363e-06</td>\n",
       "      <td>2.827232e-06</td>\n",
       "      <td>3.273111e-06</td>\n",
       "      <td>3.790622e-06</td>\n",
       "      <td>3.471280e-06</td>\n",
       "      <td>3.870206e-06</td>\n",
       "      <td>3.899283e-06</td>\n",
       "      <td>4.638921e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>1.619495e-05</td>\n",
       "      <td>1.190388e-05</td>\n",
       "      <td>6.966325e-06</td>\n",
       "      <td>3.119540e-05</td>\n",
       "      <td>3.266418e-06</td>\n",
       "      <td>2.993350e-05</td>\n",
       "      <td>2.686003e-05</td>\n",
       "      <td>8.081949e-06</td>\n",
       "      <td>3.553618e-05</td>\n",
       "      <td>3.024965e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1.427062e-07</td>\n",
       "      <td>2.176826e-07</td>\n",
       "      <td>3.450018e-07</td>\n",
       "      <td>3.515493e-07</td>\n",
       "      <td>6.909718e-07</td>\n",
       "      <td>9.051199e-07</td>\n",
       "      <td>7.228435e-07</td>\n",
       "      <td>6.703472e-07</td>\n",
       "      <td>6.452314e-07</td>\n",
       "      <td>9.593161e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               600           601           602           603           604  \\\n",
       "191   1.162032e-06  1.426706e-06  3.316030e-06  3.999727e-06  2.659342e-06   \n",
       "1353  5.100069e-06  2.961782e-06  5.754323e-06  6.631708e-06  5.013103e-06   \n",
       "1992  2.228858e-06  2.319863e-06  2.620363e-06  2.827232e-06  3.273111e-06   \n",
       "1504  1.619495e-05  1.190388e-05  6.966325e-06  3.119540e-05  3.266418e-06   \n",
       "1390  1.427062e-07  2.176826e-07  3.450018e-07  3.515493e-07  6.909718e-07   \n",
       "\n",
       "               605           606           607           608           609  \\\n",
       "191   1.188642e-06  1.898837e-06  4.202299e-06  5.332507e-06  3.882308e-06   \n",
       "1353  8.982251e-06  8.297434e-06  1.184840e-05  8.661218e-06  2.002770e-05   \n",
       "1992  3.790622e-06  3.471280e-06  3.870206e-06  3.899283e-06  4.638921e-06   \n",
       "1504  2.993350e-05  2.686003e-05  8.081949e-06  3.553618e-05  3.024965e-05   \n",
       "1390  9.051199e-07  7.228435e-07  6.703472e-07  6.452314e-07  9.593161e-07   \n",
       "\n",
       "      ...       990       991       992       993       994       995  \\\n",
       "191   ...  0.000006  0.000005  0.000004  0.000005  0.000006  0.000005   \n",
       "1353  ...  0.000015  0.000015  0.000012  0.000013  0.000013  0.000009   \n",
       "1992  ...  0.000004  0.000004  0.000004  0.000005  0.000004  0.000004   \n",
       "1504  ...  0.000020  0.000015  0.000018  0.000017  0.000014  0.000022   \n",
       "1390  ...  0.000011  0.000012  0.000012  0.000011  0.000011  0.000010   \n",
       "\n",
       "           996       997       998       999  \n",
       "191   0.000004  0.000003  0.000004  0.000005  \n",
       "1353  0.000012  0.000011  0.000013  0.000010  \n",
       "1992  0.000004  0.000005  0.000004  0.000004  \n",
       "1504  0.000012  0.000016  0.000017  0.000012  \n",
       "1390  0.000010  0.000011  0.000011  0.000010  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c61cd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191     0\n",
       "1353    1\n",
       "1992    1\n",
       "1504    1\n",
       "1390    0\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91bbfcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a3c4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60f03e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 64\n",
    "epochs = 100\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "647fcae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(neurons, input_dim = input_dim, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d0fc1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "256/256 [==============================] - 2s 4ms/step - loss: 0.6712 - accuracy: 0.6009 - val_loss: 0.6695 - val_accuracy: 0.5855\n",
      "Epoch 2/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.6567 - accuracy: 0.6021 - val_loss: 0.6544 - val_accuracy: 0.5855\n",
      "Epoch 3/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.5995 - val_loss: 0.6369 - val_accuracy: 0.6244\n",
      "Epoch 4/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6193 - val_loss: 0.6201 - val_accuracy: 0.6406\n",
      "Epoch 5/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.6073 - accuracy: 0.6546 - val_loss: 0.6049 - val_accuracy: 0.6527\n",
      "Epoch 6/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.5943 - accuracy: 0.6643 - val_loss: 0.5921 - val_accuracy: 0.6678\n",
      "Epoch 7/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.5826 - accuracy: 0.6781 - val_loss: 0.5806 - val_accuracy: 0.6914\n",
      "Epoch 8/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.5708 - accuracy: 0.6977 - val_loss: 0.5710 - val_accuracy: 0.6758\n",
      "Epoch 9/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.5601 - accuracy: 0.7066 - val_loss: 0.5599 - val_accuracy: 0.7253\n",
      "Epoch 10/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.5510 - accuracy: 0.7222 - val_loss: 0.5497 - val_accuracy: 0.7335\n",
      "Epoch 11/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.5403 - accuracy: 0.7309 - val_loss: 0.5409 - val_accuracy: 0.7345\n",
      "Epoch 12/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.5312 - accuracy: 0.7424 - val_loss: 0.5311 - val_accuracy: 0.7519\n",
      "Epoch 13/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.5228 - accuracy: 0.7481 - val_loss: 0.5224 - val_accuracy: 0.7586\n",
      "Epoch 14/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.5138 - accuracy: 0.7637 - val_loss: 0.5140 - val_accuracy: 0.7628\n",
      "Epoch 15/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.5052 - accuracy: 0.7686 - val_loss: 0.5060 - val_accuracy: 0.7665\n",
      "Epoch 16/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4980 - accuracy: 0.7818 - val_loss: 0.4983 - val_accuracy: 0.7747\n",
      "Epoch 17/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4917 - accuracy: 0.7900 - val_loss: 0.4915 - val_accuracy: 0.7925\n",
      "Epoch 18/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4840 - accuracy: 0.7926 - val_loss: 0.4869 - val_accuracy: 0.8121\n",
      "Epoch 19/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4777 - accuracy: 0.8040 - val_loss: 0.4888 - val_accuracy: 0.7945\n",
      "Epoch 20/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4717 - accuracy: 0.8066 - val_loss: 0.4775 - val_accuracy: 0.8148\n",
      "Epoch 21/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.4658 - accuracy: 0.8107 - val_loss: 0.4666 - val_accuracy: 0.8059\n",
      "Epoch 22/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.4601 - accuracy: 0.8197 - val_loss: 0.4613 - val_accuracy: 0.8257\n",
      "Epoch 23/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4555 - accuracy: 0.8214 - val_loss: 0.4550 - val_accuracy: 0.8265\n",
      "Epoch 24/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4513 - accuracy: 0.8267 - val_loss: 0.4503 - val_accuracy: 0.8339\n",
      "Epoch 25/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4463 - accuracy: 0.8295 - val_loss: 0.4499 - val_accuracy: 0.8166\n",
      "Epoch 26/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4424 - accuracy: 0.8322 - val_loss: 0.4420 - val_accuracy: 0.8428\n",
      "Epoch 27/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4380 - accuracy: 0.8360 - val_loss: 0.4376 - val_accuracy: 0.8488\n",
      "Epoch 28/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4338 - accuracy: 0.8409 - val_loss: 0.4326 - val_accuracy: 0.8518\n",
      "Epoch 29/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.4298 - accuracy: 0.8442 - val_loss: 0.4289 - val_accuracy: 0.8475\n",
      "Epoch 30/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4275 - accuracy: 0.8447 - val_loss: 0.4256 - val_accuracy: 0.8580\n",
      "Epoch 31/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4230 - accuracy: 0.8496 - val_loss: 0.4222 - val_accuracy: 0.8609\n",
      "Epoch 32/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4197 - accuracy: 0.8514 - val_loss: 0.4183 - val_accuracy: 0.8565\n",
      "Epoch 33/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4155 - accuracy: 0.8551 - val_loss: 0.4171 - val_accuracy: 0.8490\n",
      "Epoch 34/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4153 - accuracy: 0.8536 - val_loss: 0.4122 - val_accuracy: 0.8587\n",
      "Epoch 35/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4109 - accuracy: 0.8570 - val_loss: 0.4117 - val_accuracy: 0.8738\n",
      "Epoch 36/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4100 - accuracy: 0.8529 - val_loss: 0.4066 - val_accuracy: 0.8632\n",
      "Epoch 37/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4053 - accuracy: 0.8613 - val_loss: 0.4161 - val_accuracy: 0.8634\n",
      "Epoch 38/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4036 - accuracy: 0.8619 - val_loss: 0.4017 - val_accuracy: 0.8656\n",
      "Epoch 39/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.4004 - accuracy: 0.8635 - val_loss: 0.4020 - val_accuracy: 0.8778\n",
      "Epoch 40/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3979 - accuracy: 0.8654 - val_loss: 0.3966 - val_accuracy: 0.8686\n",
      "Epoch 41/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3959 - accuracy: 0.8658 - val_loss: 0.3955 - val_accuracy: 0.8617\n",
      "Epoch 42/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3929 - accuracy: 0.8675 - val_loss: 0.3918 - val_accuracy: 0.8694\n",
      "Epoch 43/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3901 - accuracy: 0.8679 - val_loss: 0.3908 - val_accuracy: 0.8627\n",
      "Epoch 44/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3882 - accuracy: 0.8696 - val_loss: 0.3972 - val_accuracy: 0.8721\n",
      "Epoch 45/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3859 - accuracy: 0.8705 - val_loss: 0.3853 - val_accuracy: 0.8686\n",
      "Epoch 46/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3838 - accuracy: 0.8700 - val_loss: 0.3830 - val_accuracy: 0.8728\n",
      "Epoch 47/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3822 - accuracy: 0.8708 - val_loss: 0.3850 - val_accuracy: 0.8642\n",
      "Epoch 48/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3787 - accuracy: 0.8734 - val_loss: 0.3855 - val_accuracy: 0.8775\n",
      "Epoch 49/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3766 - accuracy: 0.8729 - val_loss: 0.3952 - val_accuracy: 0.8694\n",
      "Epoch 50/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3747 - accuracy: 0.8746 - val_loss: 0.3808 - val_accuracy: 0.8795\n",
      "Epoch 51/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3727 - accuracy: 0.8761 - val_loss: 0.3829 - val_accuracy: 0.8773\n",
      "Epoch 52/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3714 - accuracy: 0.8751 - val_loss: 0.3719 - val_accuracy: 0.8813\n",
      "Epoch 53/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3672 - accuracy: 0.8764 - val_loss: 0.3688 - val_accuracy: 0.8808\n",
      "Epoch 54/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3661 - accuracy: 0.8748 - val_loss: 0.3663 - val_accuracy: 0.8788\n",
      "Epoch 55/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3644 - accuracy: 0.8773 - val_loss: 0.3676 - val_accuracy: 0.8818\n",
      "Epoch 56/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3617 - accuracy: 0.8770 - val_loss: 0.3637 - val_accuracy: 0.8726\n",
      "Epoch 57/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3609 - accuracy: 0.8779 - val_loss: 0.3615 - val_accuracy: 0.8840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3574 - accuracy: 0.8795 - val_loss: 0.3625 - val_accuracy: 0.8835\n",
      "Epoch 59/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3549 - accuracy: 0.8819 - val_loss: 0.3575 - val_accuracy: 0.8850\n",
      "Epoch 60/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3531 - accuracy: 0.8801 - val_loss: 0.3587 - val_accuracy: 0.8850\n",
      "Epoch 61/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3509 - accuracy: 0.8838 - val_loss: 0.3607 - val_accuracy: 0.8857\n",
      "Epoch 62/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3485 - accuracy: 0.8830 - val_loss: 0.3500 - val_accuracy: 0.8867\n",
      "Epoch 63/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3462 - accuracy: 0.8846 - val_loss: 0.3471 - val_accuracy: 0.8857\n",
      "Epoch 64/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3450 - accuracy: 0.8852 - val_loss: 0.3481 - val_accuracy: 0.8894\n",
      "Epoch 65/100\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 0.3422 - accuracy: 0.8869 - val_loss: 0.3488 - val_accuracy: 0.8805\n",
      "Epoch 66/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3399 - accuracy: 0.8874 - val_loss: 0.3521 - val_accuracy: 0.8902\n",
      "Epoch 67/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3375 - accuracy: 0.8880 - val_loss: 0.3416 - val_accuracy: 0.8852\n",
      "Epoch 68/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3354 - accuracy: 0.8891 - val_loss: 0.3385 - val_accuracy: 0.8907\n",
      "Epoch 69/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3326 - accuracy: 0.8933 - val_loss: 0.3346 - val_accuracy: 0.8902\n",
      "Epoch 70/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3311 - accuracy: 0.8919 - val_loss: 0.3381 - val_accuracy: 0.8944\n",
      "Epoch 71/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3290 - accuracy: 0.8921 - val_loss: 0.3334 - val_accuracy: 0.8961\n",
      "Epoch 72/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3254 - accuracy: 0.8926 - val_loss: 0.3299 - val_accuracy: 0.8929\n",
      "Epoch 73/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3232 - accuracy: 0.8973 - val_loss: 0.3247 - val_accuracy: 0.8927\n",
      "Epoch 74/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3206 - accuracy: 0.8967 - val_loss: 0.3226 - val_accuracy: 0.8924\n",
      "Epoch 75/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3174 - accuracy: 0.8993 - val_loss: 0.3336 - val_accuracy: 0.8944\n",
      "Epoch 76/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3174 - accuracy: 0.9004 - val_loss: 0.3186 - val_accuracy: 0.9046\n",
      "Epoch 77/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3134 - accuracy: 0.9022 - val_loss: 0.3230 - val_accuracy: 0.9013\n",
      "Epoch 78/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3105 - accuracy: 0.9026 - val_loss: 0.3130 - val_accuracy: 0.9085\n",
      "Epoch 79/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3085 - accuracy: 0.9049 - val_loss: 0.3101 - val_accuracy: 0.9051\n",
      "Epoch 80/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3070 - accuracy: 0.9048 - val_loss: 0.3078 - val_accuracy: 0.9088\n",
      "Epoch 81/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3039 - accuracy: 0.9062 - val_loss: 0.3130 - val_accuracy: 0.9008\n",
      "Epoch 82/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.3016 - accuracy: 0.9111 - val_loss: 0.3057 - val_accuracy: 0.9085\n",
      "Epoch 83/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2996 - accuracy: 0.9089 - val_loss: 0.3093 - val_accuracy: 0.9060\n",
      "Epoch 84/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2972 - accuracy: 0.9092 - val_loss: 0.2976 - val_accuracy: 0.9127\n",
      "Epoch 85/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2939 - accuracy: 0.9128 - val_loss: 0.2986 - val_accuracy: 0.9090\n",
      "Epoch 86/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2911 - accuracy: 0.9127 - val_loss: 0.2936 - val_accuracy: 0.9150\n",
      "Epoch 87/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2885 - accuracy: 0.9166 - val_loss: 0.2912 - val_accuracy: 0.9160\n",
      "Epoch 88/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2863 - accuracy: 0.9156 - val_loss: 0.2885 - val_accuracy: 0.9172\n",
      "Epoch 89/100\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.2835 - accuracy: 0.9197 - val_loss: 0.2882 - val_accuracy: 0.9152\n",
      "Epoch 90/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2809 - accuracy: 0.9193 - val_loss: 0.2840 - val_accuracy: 0.9202\n",
      "Epoch 91/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2777 - accuracy: 0.9217 - val_loss: 0.2886 - val_accuracy: 0.9152\n",
      "Epoch 92/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2764 - accuracy: 0.9201 - val_loss: 0.2811 - val_accuracy: 0.9187\n",
      "Epoch 93/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2730 - accuracy: 0.9227 - val_loss: 0.2782 - val_accuracy: 0.9212\n",
      "Epoch 94/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2720 - accuracy: 0.9225 - val_loss: 0.2754 - val_accuracy: 0.9217\n",
      "Epoch 95/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2676 - accuracy: 0.9237 - val_loss: 0.2766 - val_accuracy: 0.9189\n",
      "Epoch 96/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2671 - accuracy: 0.9256 - val_loss: 0.2686 - val_accuracy: 0.9261\n",
      "Epoch 97/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2632 - accuracy: 0.9274 - val_loss: 0.2667 - val_accuracy: 0.9256\n",
      "Epoch 98/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2606 - accuracy: 0.9280 - val_loss: 0.2649 - val_accuracy: 0.9264\n",
      "Epoch 99/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2580 - accuracy: 0.9270 - val_loss: 0.2649 - val_accuracy: 0.9246\n",
      "Epoch 100/100\n",
      "256/256 [==============================] - 1s 4ms/step - loss: 0.2559 - accuracy: 0.9287 - val_loss: 0.2623 - val_accuracy: 0.9276\n",
      "96/96 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=epochs, verbose=1, validation_split=0.33)\n",
    "predictions= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8bb2f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (predictions > 0.5) *1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4be28894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.9277\n",
      "[0.26437515020370483, 0.9276832342147827]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test,y_test, verbose=1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d862e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnew, testnew = train_test_split(testing, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2c43469",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testnew = testnew.iloc[:,0:400]\n",
    "y_testnew = testnew['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2cbbebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 1.5739 - accuracy: 0.6342\n",
      "[1.5739222764968872, 0.6342105269432068]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_testnew,y_testnew, verbose=1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd20cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05000e68",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c105715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesRegressor()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing out feature importances with ExtraTrees Regressor Model\n",
    "\n",
    "regmod = ExtraTreesRegressor()\n",
    "regmod.fit(X_train,y_train)\n",
    "\n",
    "#print(regmod.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5cb1d933",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regmod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m feat_importances \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[43mregmod\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_importances_, index\u001b[38;5;241m=\u001b[39mfaxis)\n\u001b[0;32m      3\u001b[0m feat_importances\u001b[38;5;241m.\u001b[39mnlargest(\u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbarh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'regmod' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "feat_importances = pd.Series(regmod.feature_importances_, index=faxis)\n",
    "feat_importances.nlargest(50).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7334122",
   "metadata": {},
   "source": [
    "#### The above feature importances relate to the ADC data columns directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "83c229ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['600', '601', '602', '603', '604', '605', '606', '607', '608', '609',\n",
       "       ...\n",
       "       '990', '991', '992', '993', '994', '995', '996', '997', '998', '999'],\n",
       "      dtype='object', length=400)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f6a563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a frequency axis\n",
    "interval_size = 1953125/32768\n",
    "faxis = np.linspace(int(600*interval_size), int(1000*interval_size), 400)\n",
    "faxis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb48558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
